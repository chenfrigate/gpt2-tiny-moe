import torch
import torch.nn.functional as F
from transformers import GPT2Model, GPT2Tokenizer
from models.gpt2_moe import MoETransformer

# åŠ è½½ GPT-2 Tiny é¢„è®­ç»ƒæ¨¡å‹
print("âœ… åŠ è½½ GPT-2 Tiny é¢„è®­ç»ƒå‚æ•°...")
gpt2_tiny = GPT2Model.from_pretrained("gpt2")
moe_model = MoETransformer(gpt2_tiny)

# åŠ è½½ Tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# ç¤ºä¾‹æ–‡æœ¬
text = "Hello, how are you?"
input_ids = tokenizer.encode(text, return_tensors="pt")
input_ids = input_ids[:, :10]

# è®­ç»ƒè®¾ç½®
optimizer = torch.optim.Adam(moe_model.parameters(), lr=1e-3)

print("âœ… å¼€å§‹è®­ç»ƒ MoE Transformer...")
for epoch in range(5):
    logits = moe_model(input_ids)
    target_ids = input_ids[:, 1:]
    loss = F.cross_entropy(logits[:, :-1].reshape(-1, 50257), target_ids.reshape(-1))

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
